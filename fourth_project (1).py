# -*- coding: utf-8 -*-
"""fourth project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z33wwzkOPE5kLn8V0qgPGARmEfLyQXA0
"""

import os
import math
import random
import numpy as np
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error

from docx import Document
from docx.shared import Inches



# --------------------------
# 1) DATA GENERATION: VAR-like multivariate time series
# --------------------------
def generate_var_data(n_steps=1800, n_features=5, noise_scale=0.05, seed=SEED):
    rng = np.random.RandomState(seed)
    A1 = rng.randn(n_features, n_features) * 0.28
    A2 = rng.randn(n_features, n_features) * 0.14
    data = np.zeros((n_steps, n_features), dtype=np.float32)
    data[0:2] = rng.randn(2, n_features) * 0.5
    for t in range(2, n_steps):
        data[t] = data[t-1].dot(A1.T) + data[t-2].dot(A2.T) + rng.randn(n_features) * noise_scale
    return data

data = generate_var_data(n_steps=1800, n_features=5, noise_scale=0.05)
df = pd.DataFrame(data, columns=[f"f{i+1}" for i in range(data.shape[1])])
csv_path = os.path.join(OUT_DIR, "synthetic_multivariate.csv")
df.to_csv(csv_path, index=False)
print("Saved synthetic CSV:", csv_path)

# --------------------------
# 2) CREATE SEQUENCES
# --------------------------
INPUT_LEN = 30
OUTPUT_LEN = 10   # forecast horizon
TARGET_IDX = 0    # predict feature f1 (index 0)

def create_sequences(series, input_len=30, output_len=10, target_idx=0):
    X, y = [], []
    L = len(series)
    for i in range(L - input_len - output_len + 1):
        X.append(series[i:i+input_len].astype(np.float32))
        y.append(series[i+input_len:i+input_len+output_len, target_idx].astype(np.float32))
    return np.array(X), np.array(y)

X, y = create_sequences(data, INPUT_LEN, OUTPUT_LEN, TARGET_IDX)
n_samples = len(X)
print("Total sequences:", n_samples)

# Split: 70% train, 15% val, 15% test
train_end = int(0.7 * n_samples)
val_end = int(0.85 * n_samples)

X_train, X_val, X_test = X[:train_end], X[train_end:val_end], X[val_end:]
y_train, y_val, y_test = y[:train_end], y[train_end:val_end], y[val_end:]

# Scale features (fit on train)
scaler = StandardScaler()
X_train_2d = X_train.reshape(-1, X_train.shape[-1])
scaler.fit(X_train_2d)

def scale_X(X_arr):
    flat = X_arr.reshape(-1, X_arr.shape[-1])
    transformed = scaler.transform(flat).reshape(X_arr.shape)
    return transformed

X_train_s = scale_X(X_train)
X_val_s = scale_X(X_val)
X_test_s = scale_X(X_test)

# scale targets using target feature mean/std from scaler
target_mean = scaler.mean_[TARGET_IDX]
target_std = np.sqrt(scaler.var_[TARGET_IDX])
def scale_y(y_arr):
    return (y_arr - target_mean) / target_std
def unscale_y(y_s):
    return y_s * target_std + target_mean

y_train_s = scale_y(y_train)
y_val_s = scale_y(y_val)
y_test_s = scale_y(y_test)

# --------------------------
# 3) PyTorch Dataset & DataLoader
# --------------------------
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)
    def __len__(self):
        return len(self.X)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

BATCH_SIZE = 64
train_ds = TimeSeriesDataset(X_train_s, y_train_s)
val_ds = TimeSeriesDataset(X_val_s, y_val_s)
test_ds = TimeSeriesDataset(X_test_s, y_test_s)

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)

# --------------------------
# 4) MODELS: Baseline LSTM & Seq2Seq w/ Bahdanau attention
# --------------------------
class BaselineLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers=1, dropout=0.0):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)
        self.out = nn.Linear(hidden_dim, OUTPUT_LEN)
    def forward(self, x):
        # x: (batch, seq_len, features)
        _, (h_n, _) = self.lstm(x)  # h_n: (num_layers, batch, hidden)
        h = h_n[-1]  # (batch, hidden)
        out = self.out(h)  # (batch, OUTPUT_LEN)
        return out

class BahdanauAttention(nn.Module):
    def __init__(self, enc_dim, dec_dim):
        super().__init__()
        self.W1 = nn.Linear(enc_dim, dec_dim, bias=False)
        self.W2 = nn.Linear(dec_dim, dec_dim, bias=False)
        self.V = nn.Linear(dec_dim, 1, bias=False)
    def forward(self, enc_outputs, dec_hidden):
        # enc_outputs: (batch, seq_len, enc_dim)
        # dec_hidden: (batch, dec_dim)
        # score: (batch, seq_len, 1)
        # compute W1(enc_outputs) + W2(dec_hidden) (with broadcasting)
        W1e = self.W1(enc_outputs)                # (batch, seq_len, dec_dim)
        W2d = self.W2(dec_hidden).unsqueeze(1)    # (batch, 1, dec_dim)
        score = self.V(torch.tanh(W1e + W2d))     # (batch, seq_len, 1)
        attn_weights = torch.softmax(score, dim=1)   # (batch, seq_len, 1)
        context = torch.sum(attn_weights * enc_outputs, dim=1)  # (batch, enc_dim)
        attn_weights = attn_weights.squeeze(-1)  # (batch, seq_len)
        return context, attn_weights

class Seq2SeqAttention(nn.Module):
    def __init__(self, input_dim, enc_hidden, dec_hidden, num_layers=1):
        super().__init__()
        self.encoder = nn.LSTM(input_dim, enc_hidden, num_layers=num_layers, batch_first=True)
        # we'll use a small decoder that uses encoder last hidden to compute attention and then predicts horizon
        self.attention = BahdanauAttention(enc_hidden, dec_hidden)
        self.combine = nn.Linear(enc_hidden + enc_hidden, dec_hidden)  # combine context + enc_h
        self.out = nn.Linear(dec_hidden, OUTPUT_LEN)
    def forward(self, x):
        # x: (batch, seq_len, input_dim)
        enc_outputs, (enc_h, enc_c) = self.encoder(x)  # enc_outputs: (batch, seq_len, enc_hidden)
        enc_h_last = enc_h[-1]  # (batch, enc_hidden)
        # compute context vector via attention using enc_h_last as "decoder hidden"
        context, attn_weights = self.attention(enc_outputs, enc_h_last)
        merged = torch.cat([context, enc_h_last], dim=-1)  # (batch, enc_hidden*2)
        merged_proj = torch.tanh(self.combine(merged))     # (batch, dec_hidden)
        out = self.out(merged_proj)                        # (batch, OUTPUT_LEN)
        return out, attn_weights  # attn_weights shape: (batch, seq_len)

# --------------------------
# 5) Training & evaluation helpers
# --------------------------
def train_epoch(model, loader, optim, criterion):
    model.train()
    total_loss = 0.0
    for xb, yb in loader:
        xb = xb.to(DEVICE); yb = yb.to(DEVICE)
        optim.zero_grad()
        if isinstance(model, Seq2SeqAttention) or isinstance(model, Seq2SeqAttention):
            preds, _ = model(xb)
        else:
            preds = model(xb)
        loss = criterion(preds, yb)
        loss.backward()
        optim.step()
        total_loss += loss.item() * xb.size(0)
    return total_loss / len(loader.dataset)

def eval_epoch(model, loader, criterion):
    model.eval()
    total_loss = 0.0
    preds_list = []
    ys_list = []
    attn_list = []
    with torch.no_grad():
        for xb, yb in loader:
            xb = xb.to(DEVICE); yb = yb.to(DEVICE)
            if isinstance(model, Seq2SeqAttention) or isinstance(model, Seq2SeqAttention):
                preds, attn = model(xb)
                attn_list.append(attn.detach().cpu().numpy())
            else:
                preds = model(xb)
            total_loss += criterion(preds, yb).item() * xb.size(0)
            preds_list.append(preds.detach().cpu().numpy())
            ys_list.append(yb.detach().cpu().numpy())
    preds_all = np.vstack(preds_list)
    ys_all = np.vstack(ys_list)
    attn_all = np.concatenate(attn_list, axis=0) if len(attn_list) > 0 else None
    return total_loss / len(loader.dataset), preds_all, ys_all, attn_all

def compute_metrics(y_true, y_pred):
    # y_true, y_pred are numpy arrays shape (n_samples, OUTPUT_LEN)
    mse = mean_squared_error(y_true.flatten(), y_pred.flatten())
    rmse = math.sqrt(mse)
    mae = mean_absolute_error(y_true.flatten(), y_pred.flatten())
    # MAPE - avoid divide by zero
    denom = np.where(np.abs(y_true) < 1e-6, 1e-6, y_true)
    mape = np.mean(np.abs((y_true - y_pred) / denom)) * 100.0
    return {"rmse": rmse, "mae": mae, "mape_pct": mape}

# --------------------------
# 6) Small hyperparameter sweep & training
# --------------------------
results = []
models_dir = os.path.join(OUT_DIR, "models")
os.makedirs(models_dir, exist_ok=True)

param_grid = [
    {"model": "Baseline", "hidden": 64, "lr": 1e-3, "epochs": 12},
    {"model": "Baseline", "hidden": 128, "lr": 5e-4, "epochs": 12},
    {"model": "Seq2Seq", "enc_hidden": 64, "dec_hidden": 64, "lr": 1e-3, "epochs": 12},
    {"model": "Seq2Seq", "enc_hidden": 128, "dec_hidden": 128, "lr": 5e-4, "epochs": 12},
]

criterion = nn.MSELoss()

for run_idx, p in enumerate(param_grid):
    print("\nRun", run_idx, p)
    if p["model"] == "Baseline":
        model = BaselineLSTM(input_dim=X_train_s.shape[-1], hidden_dim=p["hidden"]).to(DEVICE)
    else:
        model = Seq2SeqAttention(input_dim=X_train_s.shape[-1], enc_hidden=p["enc_hidden"], dec_hidden=p["dec_hidden"]).to(DEVICE)

    optimizer = torch.optim.Adam(model.parameters(), lr=p["lr"])
    best_val_loss = float("inf")
    epochs_no_improve = 0
    best_state = None

    for epoch in range(p["epochs"]):
        train_loss = train_epoch(model, train_loader, optimizer, criterion)
        val_loss, _, _, _ = eval_epoch(model, val_loader, criterion)
        print(f" epoch {epoch+1}/{p['epochs']}: train_loss={train_loss:.6f} val_loss={val_loss:.6f}")
        # early stopping small patience
        if val_loss < best_val_loss - 1e-6:
            best_val_loss = val_loss
            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= 4:
                print(" Early stopping (patience=4)")
                break

    # restore best state
    if best_state is not None:
        model.load_state_dict({k: v.to(DEVICE) for k, v in best_state.items()})

    # test eval
    test_loss, preds_s, y_s, attn_test = eval_epoch(model, test_loader, criterion)

    # unscale predictions and targets
    preds_un = unscale_y(preds_s)
    y_un = unscale_y(y_s)

    metrics = compute_metrics(y_un, preds_un)
    print("Test metrics:", metrics)

    # save model
    model_fname = os.path.join(models_dir, f"run_{run_idx}_{p['model']}.pt")
    torch.save(model.state_dict(), model_fname)
    print("Saved model:", model_fname)

    # save attention of this run if available
    attn_path = None
    if attn_test is not None:
        # attn_test shape: (n_samples, seq_len)
        # Save average attention heatmap across samples (we'll expand to (decoder_steps x encoder_timesteps) by repeating)
        avg_attn = np.mean(attn_test, axis=0)  # (seq_len,)
        # create an image showing same weights across OUTPUT_LEN decoder steps for easy visualization
        attn_2d = np.tile(avg_attn[np.newaxis, :], (OUTPUT_LEN, 1))  # (OUTPUT_LEN, seq_len)
        plt.figure(figsize=(8,4))
        plt.imshow(attn_2d, aspect='auto')
        plt.colorbar()
        plt.xlabel("Encoder timestep")
        plt.ylabel("Decoder step")
        plt.title(f"Avg Attention Run {run_idx}")
        attn_path = os.path.join(OUT_DIR, f"attn_run_{run_idx}.png")
        plt.savefig(attn_path, bbox_inches='tight')
        plt.close()
        print("Saved attention figure:", attn_path)

    results.append({
        "run": run_idx,
        "model": p["model"],
        "params": p,
        "metrics": metrics,
        "model_path": model_fname,
        "attn_path": attn_path
    })

# --------------------------
# 7) Save metrics table
# --------------------------
metrics_rows = []
for r in results:
    metrics_rows.append({
        "run": r["run"],
        "model": r["model"],
        "params": str(r["params"]),
        "rmse": r["metrics"]["rmse"],
        "mae": r["metrics"]["mae"],
        "mape_pct": r["metrics"]["mape_pct"],
        "model_path": r["model_path"],
        "attn_path": r["attn_path"] or ""
    })
metrics_df = pd.DataFrame(metrics_rows)
metrics_csv = os.path.join(OUT_DIR, "metrics_summary.csv")
metrics_df.to_csv(metrics_csv, index=False)
print("Saved metrics CSV:", metrics_csv)



report.add_heading("Model architectures", level=2)
report.add_paragraph("Baseline LSTM: LSTM encoder that returns last hidden state -> Dense to predict the full horizon.")
report.add_paragraph("Seq2Seq with Bahdanau attention: LSTM encoder (full sequence outputs). A Bahdanau attention module "
                     "computes a context vector using the encoder outputs and the encoder final hidden state; the context and hidden "
                     "are combined to predict the full horizon. This preserves interpretability for attention.")

report.add_heading("Hyperparameter sweep", level=2)
report.add_paragraph("A small grid of architectures/learning rates was trained. For production-grade tuning, use Optuna or another tuner and run more epochs/seeds.")

report.add_heading("Comparative performance (test set)", level=2)
table = report.add_table(rows=1, cols=6)
hdr = table.rows[0].cells
hdr[0].text = "run"
hdr[1].text = "model"
hdr[2].text = "params"
hdr[3].text = "rmse"
hdr[4].text = "mae"
hdr[5].text = "mape_pct"
for _, row in metrics_df.iterrows():
    r = table.add_row().cells
    r[0].text = str(row["run"])
    r[1].text = str(row["model"])
    r[2].text = str(row["params"])
    r[3].text = f"{row['rmse']:.6f}"
    r[4].text = f"{row['mae']:.6f}"
    r[5].text = f"{row['mape_pct']:.2f}%"

report.add_heading("Attention Visualizations", level=2)
# Insert any saved attention figs from seq runs
for r in results:
    if r["attn_path"]:
        try:
            report.add_picture(r["attn_path"], width=Inches(6))
            report.add_paragraph(f"Attention figure for run {r['run']} ({r['model']})")
        except Exception as e:
            report.add_paragraph(f"Could not embed attention {r['attn_path']}: {e}")

report.add_heading("Uploaded screenshots (user-provided)", level=2)
for sp in SCREENSHOT_PATHS:
    if os.path.exists(sp):
        try:
            report.add_picture(sp, width=Inches(6))
        except Exception as e:
            report.add_paragraph(f"Could not embed {sp}: {e}")
    else:
        report.add_paragraph(f"Screenshot not found at {sp}")

report.add_heading("Saved artifacts", level=2)
report.add_paragraph("\n".join([
    f"- Synthetic CSV: {csv_path}",
    f"- Metrics CSV: {metrics_csv}",
    f"- Models dir: {models_dir}",
    f"- Output directory: {os.path.abspath(OUT_DIR)}"
]))

report_path = os.path.join(OUT_DIR, "project_report.docx")
report.save(report_path)
print("Saved DOCX report:", report_path)